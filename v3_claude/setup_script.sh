#!/bin/bash
# ResearchMate Agent - Workflow Shell Scripts
# Execute in order: 1_ â†’ 2_ â†’ 3_ â†’ 4_ â†’ 5_ â†’ 6_

# =============================================================================
# 1_setup_environment.sh
# =============================================================================
cat > 1_setup_environment.sh << 'EOF'
#!/bin/bash
echo "ðŸ”§ Setting up ResearchMate environment..."

# Create project directory structure
mkdir -p researchmate/{models,data,logs}
cd researchmate

# Check Python version
python_version=$(python3 --version 2>&1 | grep -o "3\.[0-9]\+")
if [[ $? -ne 0 ]] || [[ $(echo "$python_version" | cut -d. -f2) -lt 8 ]]; then
    echo "âŒ Python 3.8+ required"
    exit 1
fi
echo "âœ… Python $python_version detected"

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install --upgrade pip
pip install fastapi==0.104.1 uvicorn==0.24.0 pydantic==2.5.0
pip install requests==2.31.0 chromadb==0.4.18 sentence-transformers==2.2.2
pip install arxiv==1.4.8 python-multipart==0.0.6

echo "âœ… Environment setup complete"
echo "ðŸ“ Project structure created in: $(pwd)"
echo "ðŸ Virtual environment activated"
EOF

# =============================================================================
# 2_download_model.sh
# =============================================================================
cat > 2_download_model.sh << 'EOF'
#!/bin/bash
echo "ðŸ“¥ Downloading recommended LLM model..."

cd researchmate/models

# Check available space (need ~5GB)
available_space=$(df . | tail -1 | awk '{print $4}')
if [[ $available_space -lt 5000000 ]]; then
    echo "âŒ Need ~5GB free space for model"
    exit 1
fi

# Download Llama 3.1 8B Instruct (Q4_K_M quantization)
MODEL_URL="https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"
MODEL_FILE="llama-3.1-8b-instruct-q4_k_m.gguf"

if [[ ! -f "$MODEL_FILE" ]]; then
    echo "ðŸ”½ Downloading $MODEL_FILE (~4.9GB)..."
    curl -L -o "$MODEL_FILE" "$MODEL_URL"

    if [[ $? -eq 0 ]]; then
        echo "âœ… Model downloaded: $MODEL_FILE"
        ls -lh "$MODEL_FILE"
    else
        echo "âŒ Download failed"
        echo "ðŸ’¡ Alternative: Download manually from HuggingFace"
        echo "   URL: $MODEL_URL"
        exit 1
    fi
else
    echo "âœ… Model already exists: $MODEL_FILE"
fi

echo "ðŸŽ¯ Model ready for llama.cpp server"
echo "ðŸ“ Location: $(pwd)/$MODEL_FILE"
EOF

# =============================================================================
# 3_setup_llama_cpp.sh
# =============================================================================
cat > 3_setup_llama_cpp.sh << 'EOF'
#!/bin/bash
echo "ðŸ› ï¸ Setting up llama.cpp server..."

# Check if llama.cpp already exists
if [[ -d "llama.cpp" ]]; then
    echo "ðŸ“ llama.cpp directory exists"
    cd llama.cpp

    if [[ -f "server" ]]; then
        echo "âœ… llama.cpp server already built"
        exit 0
    fi
else
    # Clone llama.cpp
    echo "ðŸ“‚ Cloning llama.cpp repository..."
    git clone https://github.com/ggerganov/llama.cpp.git
    cd llama.cpp
fi

# Build with optimizations
echo "ðŸ”¨ Building llama.cpp server..."

# Detect system and build accordingly
if [[ "$OSTYPE" == "darwin"* ]]; then
    # macOS - use Metal acceleration if available
    if system_profiler SPDisplaysDataType | grep -q "Metal"; then
        echo "ðŸš€ Building with Metal acceleration (macOS)"
        make LLAMA_METAL=1 server
    else
        echo "ðŸ”§ Building with CPU only (macOS)"
        make server
    fi
elif command -v nvidia-smi &> /dev/null; then
    # Linux with NVIDIA GPU
    echo "ðŸš€ Building with CUDA acceleration"
    make LLAMA_CUBLAS=1 server
else
    # CPU only
    echo "ðŸ”§ Building with CPU only"
    make server
fi

if [[ $? -eq 0 ]]; then
    echo "âœ… llama.cpp server built successfully"
    echo "ðŸ“ Binary location: $(pwd)/server"
else
    echo "âŒ Build failed"
    echo "ðŸ’¡ Try: make clean && make server"
    exit 1
fi
EOF

# =============================================================================
# 4_start_llm_server.sh
# =============================================================================
cat > 4_start_llm_server.sh << 'EOF'
#!/bin/bash
echo "ðŸš€ Starting llama.cpp server..."

# Find model file
MODEL_PATH=""
if [[ -f "researchmate/models/llama-3.1-8b-instruct-q4_k_m.gguf" ]]; then
    MODEL_PATH="researchmate/models/llama-3.1-8b-instruct-q4_k_m.gguf"
elif [[ -f "models/llama-3.1-8b-instruct-q4_k_m.gguf" ]]; then
    MODEL_PATH="models/llama-3.1-8b-instruct-q4_k_m.gguf"
else
    echo "âŒ Model file not found"
    echo "ðŸ’¡ Run 2_download_model.sh first"
    exit 1
fi

# Find llama.cpp server
SERVER_PATH=""
if [[ -f "llama.cpp/server" ]]; then
    SERVER_PATH="llama.cpp/server"
elif [[ -f "../llama.cpp/server" ]]; then
    SERVER_PATH="../llama.cpp/server"
else
    echo "âŒ llama.cpp server not found"
    echo "ðŸ’¡ Run 3_setup_llama_cpp.sh first"
    exit 1
fi

echo "ðŸ¤– Model: $MODEL_PATH"
echo "âš¡ Server: $SERVER_PATH"

# Check if port 8080 is available
if lsof -Pi :8080 -sTCP:LISTEN -t >/dev/null ; then
    echo "âš ï¸  Port 8080 is busy. Stopping existing process..."
    pkill -f "server.*8080" 2>/dev/null || true
    sleep 2
fi

# Start server with optimized settings
echo "ðŸŽ¯ Starting LLM server on localhost:8080..."
"$SERVER_PATH" \
    --model "$MODEL_PATH" \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --threads $(nproc 2>/dev/null || sysctl -n hw.ncpu 2>/dev/null || echo 4) \
    --batch-size 512 \
    --ubatch-size 256 \
    --log-disable \
    --metrics &

SERVER_PID=$!
echo "ðŸ“ Server PID: $SERVER_PID"
echo "$SERVER_PID" > llm_server.pid

# Wait for server to start
echo "â³ Waiting for server to initialize..."
for i in {1..30}; do
    if curl -s http://localhost:8080/health >/dev/null 2>&1; then
        echo "âœ… LLM server ready!"
        echo "ðŸŒ Health check: http://localhost:8080/health"
        echo "ðŸ”§ To stop: kill $SERVER_PID"
        exit 0
    fi
    sleep 2
    echo -n "."
done

echo "âŒ Server failed to start within 60 seconds"
echo "ðŸ” Check logs and try manual start"
exit 1
EOF

# =============================================================================
# 5_start_research_agent.sh
# =============================================================================
cat > 5_start_research_agent.sh << 'EOF'
#!/bin/bash
echo "ðŸ”¬ Starting ResearchMate Agent..."

# Activate virtual environment
if [[ -f "researchmate/venv/bin/activate" ]]; then
    source researchmate/venv/bin/activate
    cd researchmate
elif [[ -f "venv/bin/activate" ]]; then
    source venv/bin/activate
else
    echo "âŒ Virtual environment not found"
    echo "ðŸ’¡ Run 1_setup_environment.sh first"
    exit 1
fi

# Check if LLM server is running
if ! curl -s http://localhost:8080/health >/dev/null 2>&1; then
    echo "âŒ LLM server not responding on localhost:8080"
    echo "ðŸ’¡ Run 4_start_llm_server.sh first"
    exit 1
fi

# Check if ResearchMate code exists
if [[ ! -f "main.py" ]]; then
    echo "âŒ main.py not found"
    echo "ðŸ’¡ Copy the ResearchMate code to main.py"
    exit 1
fi

# Create logs directory
mkdir -p logs

# Check if port 8000 is available
if lsof -Pi :8000 -sTCP:LISTEN -t >/dev/null ; then
    echo "âš ï¸  Port 8000 is busy. Stopping existing process..."
    pkill -f "uvicorn.*8000" 2>/dev/null || true
    pkill -f "main.py" 2>/dev/null || true
    sleep 2
fi

# Start ResearchMate agent
echo "ðŸš€ Starting ResearchMate on localhost:8000..."
python main.py > logs/agent.log 2>&1 &

AGENT_PID=$!
echo "ðŸ“ Agent PID: $AGENT_PID"
echo "$AGENT_PID" > agent.pid

# Wait for agent to start
echo "â³ Waiting for agent to initialize..."
for i in {1..20}; do
    if curl -s http://localhost:8000/ >/dev/null 2>&1; then
        echo "âœ… ResearchMate Agent ready!"
        echo "ðŸŒ API: http://localhost:8000"
        echo "ðŸ“š Docs: http://localhost:8000/docs"
        echo "ðŸ”§ To stop: kill $AGENT_PID"
        exit 0
    fi
    sleep 1
    echo -n "."
done

echo "âŒ Agent failed to start within 20 seconds"
echo "ðŸ“‹ Check logs: tail logs/agent.log"
exit 1
EOF

# =============================================================================
# 6_test_workflow.sh
# =============================================================================
cat > 6_test_workflow.sh << 'EOF'
#!/bin/bash
echo "ðŸ§ª Testing ResearchMate workflow..."

# Check if agent is running
if ! curl -s http://localhost:8000/ >/dev/null 2>&1; then
    echo "âŒ ResearchMate agent not responding"
    echo "ðŸ’¡ Run 5_start_research_agent.sh first"
    exit 1
fi

# Test 1: Health check
echo "1ï¸âƒ£ Testing health endpoint..."
HEALTH=$(curl -s http://localhost:8000/)
if echo "$HEALTH" | grep -q "ResearchMate"; then
    echo "âœ… Health check passed"
else
    echo "âŒ Health check failed"
    exit 1
fi

# Test 2: Start research query
echo "2ï¸âƒ£ Starting test research query..."
QUERY='{"query": "transformer attention mechanisms", "max_papers": 3}'
RESPONSE=$(curl -s -X POST http://localhost:8000/research/query \
    -H "Content-Type: application/json" \
    -d "$QUERY")

JOB_ID=$(echo "$RESPONSE" | grep -o '"job_id":"[^"]*"' | cut -d'"' -f4)

if [[ -z "$JOB_ID" ]]; then
    echo "âŒ Failed to start research query"
    echo "Response: $RESPONSE"
    exit 1
fi

echo "âœ… Research started. Job ID: $JOB_ID"

# Test 3: Monitor job status
echo "3ï¸âƒ£ Monitoring job progress..."
for i in {1..60}; do
    STATUS_RESPONSE=$(curl -s http://localhost:8000/research/status/$JOB_ID)
    STATUS=$(echo "$STATUS_RESPONSE" | grep -o '"status":"[^"]*"' | cut -d'"' -f4)

    echo "â³ Status: $STATUS (${i}s elapsed)"

    if [[ "$STATUS" == "completed" ]]; then
        echo "âœ… Research completed!"
        break
    elif [[ "$STATUS" == "failed" ]]; then
        echo "âŒ Research failed"
        echo "Response: $STATUS_RESPONSE"
        exit 1
    fi

    sleep 2
done

if [[ "$STATUS" != "completed" ]]; then
    echo "â° Research timed out after 2 minutes"
    exit 1
fi

# Test 4: Get results
echo "4ï¸âƒ£ Retrieving results..."
RESULTS=$(curl -s http://localhost:8000/research/results/$JOB_ID)

if echo "$RESULTS" | grep -q "papers_found"; then
    PAPERS_COUNT=$(echo "$RESULTS" | grep -o '"papers_found":[0-9]*' | cut -d':' -f2)
    echo "âœ… Results retrieved: $PAPERS_COUNT papers found"

    # Save results to file
    echo "$RESULTS" | python3 -m json.tool > test_results.json
    echo "ðŸ“„ Results saved to: test_results.json"
else
    echo "âŒ Failed to retrieve results"
    echo "Response: $RESULTS"
    exit 1
fi

# Test 5: Stats check
echo "5ï¸âƒ£ Checking system stats..."
STATS=$(curl -s http://localhost:8000/stats)
if echo "$STATS" | grep -q "jobs"; then
    echo "âœ… Stats retrieved successfully"
    echo "$STATS" | python3 -m json.tool
else
    echo "âŒ Failed to get stats"
fi

echo ""
echo "ðŸŽ‰ All tests passed! ResearchMate is working correctly."
echo "ðŸ”— Try the interactive docs: http://localhost:8000/docs"
echo "ðŸ“ Example query: curl -X POST http://localhost:8000/research/query -H 'Content-Type: application/json' -d '{\"query\":\"machine learning transformers\"}'"
EOF

# =============================================================================
# 7_stop_all.sh
# =============================================================================
cat > 7_stop_all.sh << 'EOF'
#!/bin/bash
echo "ðŸ›‘ Stopping all ResearchMate services..."

# Stop ResearchMate agent
if [[ -f "agent.pid" ]]; then
    AGENT_PID=$(cat agent.pid)
    if kill -0 "$AGENT_PID" 2>/dev/null; then
        echo "ðŸ”¬ Stopping ResearchMate agent (PID: $AGENT_PID)..."
        kill "$AGENT_PID"
        rm agent.pid
    fi
fi

# Stop LLM server
if [[ -f "llm_server.pid" ]]; then
    SERVER_PID=$(cat llm_server.pid)
    if kill -0 "$SERVER_PID" 2>/dev/null; then
        echo "ðŸ¤– Stopping LLM server (PID: $SERVER_PID)..."
        kill "$SERVER_PID"
        rm llm_server.pid
    fi
fi

# Kill any remaining processes
pkill -f "uvicorn.*8000" 2>/dev/null || true
pkill -f "server.*8080" 2>/dev/null || true
pkill -f "main.py" 2>/dev/null || true

# Check if ports are free
sleep 2
if lsof -Pi :8000 -sTCP:LISTEN -t >/dev/null ; then
    echo "âš ï¸  Port 8000 still busy"
else
    echo "âœ… Port 8000 freed"
fi

if lsof -Pi :8080 -sTCP:LISTEN -t >/dev/null ; then
    echo "âš ï¸  Port 8080 still busy"
else
    echo "âœ… Port 8080 freed"
fi

echo "ðŸ All services stopped"
EOF

# =============================================================================
# 8_cleanup.sh
# =============================================================================
cat > 8_cleanup.sh << 'EOF'
#!/bin/bash
echo "ðŸ§¹ Cleaning up ResearchMate environment..."

# Stop all services first
bash 7_stop_all.sh

echo "ðŸ—‘ï¸ Removing temporary files..."

# Remove databases and caches
rm -f researchmate.db
rm -rf chroma_db/
rm -rf __pycache__/
rm -f test_results.json
rm -f *.log

# Remove logs
rm -rf logs/

# Clean Python cache
find . -name "*.pyc" -delete
find . -name "__pycache__" -type d -exec rm -rf {} + 2>/dev/null || true

echo "âœ… Cleanup complete"
echo "ðŸ’¡ To fully reset:"
echo "   - Remove models/ directory (saves ~5GB)"
echo "   - Remove llama.cpp/ directory"
echo "   - Remove venv/ directory"
EOF

# =============================================================================
# Make all scripts executable
# =============================================================================

chmod +x *.sh

echo "ðŸ“‹ ResearchMate workflow scripts created:"
echo "   1_setup_environment.sh    - Install deps & create venv"
echo "   2_download_model.sh       - Download Llama 3.1 8B model"
echo "   3_setup_llama_cpp.sh      - Build llama.cpp server"
echo "   4_start_llm_server.sh     - Start LLM server (port 8080)"
echo "   5_start_research_agent.sh - Start ResearchMate (port 8000)"
echo "   6_test_workflow.sh        - Run end-to-end test"
echo "   7_stop_all.sh            - Stop all services"
echo "   8_cleanup.sh             - Clean temporary files"
echo ""
echo "ðŸš€ Quick start: ./1_setup_environment.sh && ./2_download_model.sh && ./3_setup_llama_cpp.sh"
EOF